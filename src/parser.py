#!/usr/bin/env python3
"""
Avito Parser - –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç
–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ JSON –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
"""

import os
import sys
import json
import asyncio
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

import aiohttp
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from telegram import Bot, InlineKeyboardButton, InlineKeyboardMarkup

# ===================== –ö–û–ù–§–ò–ì =====================

TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
ADMIN_IDS = list(map(int, os.getenv('TELEGRAM_ADMIN_IDS', '').split(','))) if os.getenv('TELEGRAM_ADMIN_IDS') else []

BASE_DIR = Path(__file__).parent.parent.parent
DATA_DIR = BASE_DIR / 'data'
DATA_DIR.mkdir(exist_ok=True)

PRICES_FILE = DATA_DIR / 'prices.json'
TRENDS_FILE = DATA_DIR / 'trends.json'
SEEN_ADS_FILE = DATA_DIR / 'seen_ads.json'

# ===================== –ü–ê–†–°–ï–† =====================

class AvitoParser:
    """–ü–∞—Ä—Å–µ—Ä Avito —Å –∞–Ω—Ç–∏–±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π"""
    
    BASE_URL = "https://www.avito.ru"
    
    def __init__(self):
        self.ua = UserAgent()
    
    def _get_headers(self):
        """–†–µ–∞–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –±—Ä–∞—É–∑–µ—Ä–∞"""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
    
    async def search(self, query: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
        async with aiohttp.ClientSession() as session:
            try:
                await asyncio.sleep(random.uniform(2, 4))
                
                headers = self._get_headers()
                params = {'q': query}
                
                async with session.get(
                    f"{self.BASE_URL}/rossiya",
                    params=params,
                    headers=headers,
                    timeout=30
                ) as response:
                    
                    if response.status != 200:
                        print(f"‚ùå HTTP {response.status} for {query}")
                        return []
                    
                    html = await response.text()
                    return self._parse_results(html, limit)
                    
            except Exception as e:
                print(f"‚ùå Error: {e}")
                return []
    
    def _parse_results(self, html: str, limit: int) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        ads = []
        
        items = soup.select('[data-marker="item"]')
        
        for item in items[:limit]:
            try:
                # ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è
                ad_id = item.get('id', '')
                if not ad_id:
                    continue
                
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = item.select_one('[itemprop="name"]')
                title = title_elem.text.strip() if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
                
                # –¶–µ–Ω–∞
                price_elem = item.select_one('[itemprop="price"]')
                price = price_elem.get('content', '0') if price_elem else '0'
                
                # –°—Å—ã–ª–∫–∞
                link_elem = item.select_one('a[href*="/"]')
                if link_elem:
                    href = link_elem.get('href', '')
                    url = f"{self.BASE_URL}{href}" if href.startswith('/') else href
                else:
                    url = ""
                
                # –î–∞—Ç–∞
                date_elem = item.select_one('[data-marker="item-date"]')
                date = date_elem.text.strip() if date_elem else ""
                
                # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
                location_elem = item.select_one('[class*="address"]')
                location = location_elem.text.strip() if location_elem else ""
                
                ads.append({
                    'id': ad_id,
                    'title': title[:100],
                    'price': price,
                    'url': url,
                    'date': date,
                    'location': location,
                    'query': query,
                    'found_at': datetime.now().isoformat()
                })
                
            except Exception as e:
                print(f"‚ùå Parse error: {e}")
                continue
        
        return ads

# ===================== –ë–ê–ó–ê –î–ê–ù–ù–´–• =====================

def load_json(file: Path, default: dict = None):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å JSON"""
    if default is None:
        default = {}
    if file.exists():
        try:
            return json.loads(file.read_text(encoding='utf-8'))
        except:
            return default
    return default

def save_json(file: Path, data: dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON"""
    file.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding='utf-8')

# ===================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê =====================

def update_prices(query: str, price: float):
    """–û–±–Ω–æ–≤–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ü–µ–Ω"""
    prices = load_json(PRICES_FILE, {})
    
    if query not in prices:
        prices[query] = []
    
    prices[query].append({
        'price': price,
        'time': datetime.now().isoformat()
    })
    
    # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–Ω–∞—á–µ–Ω–∏–π
    if len(prices[query]) > 100:
        prices[query] = prices[query][-100:]
    
    save_json(PRICES_FILE, prices)

def update_trends(query: str):
    """–û–±–Ω–æ–≤–∏—Ç—å —Ç—Ä–µ–Ω–¥—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
    trends = load_json(TRENDS_FILE, {})
    
    if query not in trends:
        trends[query] = 0
    
    trends[query] += 1
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
    trends = dict(sorted(trends.items(), key=lambda x: x[1], reverse=True))
    
    save_json(TRENDS_FILE, trends)

# ===================== –£–í–ï–î–û–ú–õ–ï–ù–ò–Ø =====================

async def send_notification(bot: Bot, user_id: int, ad: Dict):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–æ–≤–æ–º –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
    try:
        price = int(float(ad['price'])) if ad['price'].replace('.', '').isdigit() else 0
        if price >= 1000:
            price_text = f"{price/1000:.0f} —Ç—ã—Å ‚ÇΩ"
        else:
            price_text = f"{price} ‚ÇΩ"
        
        keyboard = InlineKeyboardMarkup([
            [InlineKeyboardButton("üîó –û—Ç–∫—Ä—ã—Ç—å –æ–±—ä—è–≤–ª–µ–Ω–∏–µ", url=ad['url'])]
        ])
        
        text = (
            f"üÜï **–ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ!**\n\n"
            f"üîç **–ó–∞–ø—Ä–æ—Å:** {ad['query']}\n"
            f"üè∑ **{ad['title']}**\n"
            f"üí∞ **–¶–µ–Ω–∞:** {price_text}\n"
        )
        
        if ad['location']:
            text += f"üìç **–ú–µ—Å—Ç–æ:** {ad['location']}\n"
        
        await bot.send_message(
            chat_id=user_id,
            text=text,
            parse_mode='Markdown',
            reply_markup=keyboard,
            disable_web_page_preview=False
        )
        return True
    except Exception as e:
        print(f"‚ùå Send error: {e}")
        return False

# ===================== –û–°–ù–û–í–ù–û–ï =====================

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print(f"üöÄ Parser started at {datetime.now()}")
    
    if not TOKEN:
        print("‚ùå No token!")
        return
    
    bot = Bot(token=TOKEN)
    parser = AvitoParser()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
    seen_ads = load_json(SEEN_ADS_FILE, {"ads": []})
    
    # –¢–æ–ø-5 –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    trends = load_json(TRENDS_FILE, {})
    top_queries = list(trends.keys())[:5] if trends else ["iphone 13", "macbook", "ps5", "–≤–µ–ª–æ—Å–∏–ø–µ–¥", "–¥–∏–≤–∞–Ω"]
    
    print(f"üîç Checking {len(top_queries)} queries...")
    
    new_ads_count = 0
    
    for query in top_queries:
        print(f"  üìç Searching: {query}")
        
        ads = await parser.search(query, limit=3)
        
        for ad in ads:
            if ad['id'] not in seen_ads['ads']:
                seen_ads['ads'].append(ad['id'])
                new_ads_count += 1
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –∞–¥–º–∏–Ω–∞–º
                for admin_id in ADMIN_IDS:
                    await send_notification(bot, admin_id, ad)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ü–µ–Ω
                try:
                    price_val = float(ad['price'])
                    update_prices(query, price_val)
                except:
                    pass
                
                await asyncio.sleep(0.5)
        
        await asyncio.sleep(random.uniform(1, 3))
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–Ω–¥—ã
    for query in top_queries:
        update_trends(query)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ (—Ö—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 1000)
    seen_ads['ads'] = seen_ads['ads'][-1000:]
    save_json(SEEN_ADS_FILE, seen_ads)
    
    print(f"‚úÖ Found {new_ads_count} new ads")
    print(f"üèÅ Parser finished at {datetime.now()}")

if __name__ == "__main__":
    asyncio.run(main())
